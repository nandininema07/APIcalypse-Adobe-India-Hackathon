{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "327be087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2081b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "120fc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs_with_ocr(pdf_path):\n",
    "    structured_blocks = []\n",
    "    page_sizes = {}\n",
    "    order = 0\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_idx, page in enumerate(pdf.pages, 0):\n",
    "            page_sizes[str(page_idx)] = {\"width\": page.width, \"height\": page.height}\n",
    "            # Try to extract normal lines first\n",
    "            lines = list(page.extract_text_lines())\n",
    "            if lines and any(line.get(\"text\", \"\").strip() for line in lines) and len(lines)>=5:\n",
    "                # (Use your previous merging logic)\n",
    "                for line in lines:\n",
    "                    block = {\n",
    "                        \"box\": [line[\"x0\"], line[\"top\"], line[\"x1\"], line[\"bottom\"]],\n",
    "                        \"text\": line[\"text\"],\n",
    "                        \"page\": page_idx,\n",
    "                        \"id\": order,    \n",
    "                        \"order\": order\n",
    "                    }\n",
    "                    structured_blocks.append(block)\n",
    "                    order += 1\n",
    "            else:\n",
    "                # If no text, do OCR\n",
    "                pil_img = page.to_image(resolution=300).original\n",
    "                ocr_text = pytesseract.image_to_string(pil_img)\n",
    "                block = {\n",
    "                    \"box\": [0,0,page.width,page.height],\n",
    "                    \"text\": ocr_text,\n",
    "                    \"page\": page_idx,\n",
    "                    \"id\": order,\n",
    "                    \"order\": order\n",
    "                }\n",
    "                structured_blocks.append(block)\n",
    "                order += 1\n",
    "\n",
    "    result = {\n",
    "        \"contents\": structured_blocks\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"./input\"\n",
    "output_folder = \"./output\"\n",
    "\n",
    "for fname in os.listdir(input_folder):\n",
    "    if fname.lower().endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(input_folder, fname)\n",
    "        out_path = os.path.join(output_folder, os.path.splitext(fname)[0] + \".json\")\n",
    "        structured = extract_paragraphs_with_ocr(pdf_path)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(structured, f, indent=2)\n",
    "        print(f\"Extracted structure for {fname} -> {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fff60f",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Model input: \n",
    "{\n",
    "    \"box\": [_, _, _, _],\n",
    "    \"text\": \"______\",\n",
    "    \"page\": _,\n",
    "    \"id\": _\n",
    "}\n",
    "\n",
    "Model Output:\n",
    "{\n",
    "    \"label\": \"_____\",\n",
    "    \"linking\": [[_, _]],\n",
    "    \"order\": \n",
    "}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a5fc0",
   "metadata": {},
   "source": [
    "<h1>Main Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class DocIENetDataset(Dataset):\n",
    "    VALID_LABELS = {'title', 'section-title'}\n",
    "\n",
    "    def __init__(self, input_files, output_files, tokenizer, max_length=64, label_encoder=None):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.order_counts_dict = {}\n",
    "\n",
    "        if label_encoder is None:\n",
    "            all_labels = set(['title', 'section-title'])\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            self.label_encoder.fit(sorted(list(all_labels)))\n",
    "        else:\n",
    "            self.label_encoder = label_encoder\n",
    "\n",
    "        self.link_counts = 0\n",
    "        self.order_counts = 0\n",
    "        for input_file, output_file in tqdm(zip(input_files, output_files), desc=\"Loading dataset\"):\n",
    "            self._process_file_pair(input_file, output_file)\n",
    "\n",
    "        print(f\"Dataset loaded with {len(self.data)} samples\")\n",
    "        print(f\"Valid labels: {list(self.label_encoder.classes_)}\")\n",
    "        print(f\"Total valid links: {self.link_counts}\")\n",
    "        print(f\"Total items with reassigned orders: {self.order_counts}\")\n",
    "        print(f\"Order distribution in dataset: {self.order_counts_dict}\")\n",
    "\n",
    "    def _process_file_pair(self, input_file, output_file):\n",
    "        try:\n",
    "            with open(input_file, 'r', encoding='utf-8') as f:\n",
    "                input_data = json.load(f)\n",
    "            with open(output_file, 'r', encoding='utf-8') as f:\n",
    "                output_data = json.load(f)\n",
    "\n",
    "            output_map = {item['id']: item for item in output_data.get('contents', [])}\n",
    "            valid_ids = {item['id'] for item in output_data.get('contents', []) if item.get('label') in self.VALID_LABELS}\n",
    "\n",
    "            for item in input_data.get('contents', []):\n",
    "                item_id = item.get('id')\n",
    "                if item_id not in output_map:\n",
    "                    continue\n",
    "                output_item = output_map[item_id]\n",
    "                label = output_item.get('label', 'other')\n",
    "                if label not in self.VALID_LABELS:\n",
    "                    continue\n",
    "                if label not in self.label_encoder.classes_:\n",
    "                    continue\n",
    "\n",
    "                page_num = item.get('page', 1)\n",
    "                page_info = input_data.get('pages', {}).get(f'page{page_num}', \n",
    "                                                           input_data.get('pages', {}).get(str(page_num), \n",
    "                                                           {'width': 1000, 'height': 1000}))\n",
    "                box = item.get('box', [0, 0, 0, 0])\n",
    "                normalized_box = [\n",
    "                    box[0] / page_info['width'],\n",
    "                    box[1] / page_info['height'],\n",
    "                    box[2] / page_info['width'],\n",
    "                    box[3] / page_info['height']\n",
    "                ]\n",
    "\n",
    "                parent_id = 0\n",
    "                for link in output_item.get('linking', []):\n",
    "                    if len(link) == 2 and link[0] in valid_ids and link[1] in valid_ids and link[0] != 0 and link[1] != 0:\n",
    "                        parent_id = link[1]\n",
    "                        self.link_counts += 1\n",
    "                        break\n",
    "\n",
    "                if parent_id == 0:\n",
    "                    for other_item in input_data.get('contents', []):\n",
    "                        other_id = other_item.get('id')\n",
    "                        if other_id != item_id and other_id in output_map:\n",
    "                            other_label = output_map[other_id].get('label', 'other')\n",
    "                            if other_label in self.VALID_LABELS:\n",
    "                                other_box = other_item.get('box', [0, 0, 0, 0])\n",
    "                                other_box_norm = [\n",
    "                                    other_box[0] / page_info['width'],\n",
    "                                    other_box[1] / page_info['height'],\n",
    "                                    other_box[2] / page_info['width'],\n",
    "                                    other_box[3] / page_info['height']\n",
    "                                ]\n",
    "                                dist = np.sqrt((normalized_box[0] - other_box_norm[0])**2 + (normalized_box[1] - other_box_norm[1])**2)\n",
    "                                if dist < 0.2 and parent_id == 0:\n",
    "                                    parent_id = other_id\n",
    "                                    self.link_counts += 1\n",
    "                                    break\n",
    "\n",
    "                text = item.get('text', '') or '[EMPTY]'\n",
    "                order = min(output_item.get('order', 0), 6)  # Cap at 6 based on GT distribution\n",
    "                self.order_counts_dict[order] = self.order_counts_dict.get(order, 0) + 1\n",
    "\n",
    "                self.data.append({\n",
    "                    'text': text,\n",
    "                    'box': normalized_box,\n",
    "                    'page': page_num,\n",
    "                    'id': item_id,\n",
    "                    'label': label,\n",
    "                    'parent_id': parent_id,\n",
    "                    'order': order,\n",
    "                    'file_idx': len(self.data)\n",
    "                })\n",
    "                self.order_counts += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_file}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            item['text'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'box': torch.tensor(item['box'], dtype=torch.float32),\n",
    "            'page': torch.tensor(item['page'], dtype=torch.long),\n",
    "            'label': item['label'],\n",
    "            'id': item['id'],\n",
    "            'parent_id': item['parent_id'],\n",
    "            'order': torch.tensor(item['order'], dtype=torch.long),\n",
    "            'file_idx': item['file_idx']\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    box = torch.stack([item['box'] for item in batch])\n",
    "    page = torch.stack([item['page'] for item in batch])\n",
    "    order = torch.stack([item['order'] for item in batch])\n",
    "    labels = [item['label'] for item in batch]\n",
    "    ids = [item['id'] for item in batch]\n",
    "    file_idx = [item['file_idx'] for item in batch]\n",
    "    parent_ids = [item['parent_id'] for item in batch]\n",
    "\n",
    "    batch_size = len(batch)\n",
    "    id_to_idx = {id_val: i for i, id_val in enumerate(ids)}\n",
    "    linking_targets = torch.zeros(batch_size, dtype=torch.long)\n",
    "    for i, parent_id in enumerate(parent_ids):\n",
    "        if parent_id in id_to_idx:\n",
    "            linking_targets[i] = id_to_idx[parent_id]\n",
    "        else:\n",
    "            linking_targets[i] = 0  # No parent (H1)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'box': box,\n",
    "        'page': page,\n",
    "        'order': order,\n",
    "        'label': labels,\n",
    "        'id': ids,\n",
    "        'file_idx': file_idx,\n",
    "        'linking_targets': linking_targets\n",
    "    }\n",
    "\n",
    "class CompactDocumentModel(nn.Module):\n",
    "    def __init__(self, num_labels, model_name='./pretrained_models_bert_tiny'):\n",
    "        super().__init__()\n",
    "        self.text_encoder = AutoModel.from_pretrained(model_name)\n",
    "        text_dim = self.text_encoder.config.hidden_size\n",
    "\n",
    "        self.spatial_encoder = nn.Sequential(\n",
    "            nn.Linear(5, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        combined_dim = text_dim + 128\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "        self.order_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 7)  # 7 classes for orders 0-6\n",
    "        )\n",
    "\n",
    "        self.feature_projector = nn.Linear(combined_dim, 256)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, box, page):\n",
    "        text_output = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_output.last_hidden_state[:, 0]\n",
    "\n",
    "        spatial_input = torch.cat([box, page.unsqueeze(-1).float()], dim=-1)\n",
    "        spatial_features = self.spatial_encoder(spatial_input)\n",
    "\n",
    "        combined_features = torch.cat([text_features, spatial_features], dim=-1)\n",
    "        label_logits = self.classifier(combined_features)\n",
    "        order_pred = self.order_head(combined_features)\n",
    "        linking_features = self.feature_projector(combined_features)\n",
    "\n",
    "        return {\n",
    "            'label_logits': label_logits,\n",
    "            'order_pred': order_pred,\n",
    "            'features': combined_features,\n",
    "            'linking_features': linking_features\n",
    "        }\n",
    "\n",
    "class ImprovedLinkingPredictor(nn.Module):\n",
    "    def __init__(self, feature_dim=256):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(feature_dim, num_heads=8, batch_first=True)\n",
    "        self.similarity_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 3 + 3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, features, boxes):\n",
    "        batch_size, feature_dim = features.shape\n",
    "        attended_features, _ = self.attention(\n",
    "            features.unsqueeze(0), features.unsqueeze(0), features.unsqueeze(0)\n",
    "        )\n",
    "        attended_features = attended_features.squeeze(0)\n",
    "\n",
    "        features_i = attended_features.unsqueeze(1).expand(-1, batch_size, -1)\n",
    "        features_j = attended_features.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        concat_features = torch.cat([features_i, features_j], dim=-1)\n",
    "        product_features = features_i * features_j\n",
    "\n",
    "        #Finds intrinstic properties for model to learn - Focuses on layout\n",
    "        boxes_i = boxes.unsqueeze(1).expand(-1, batch_size, -1)\n",
    "        boxes_j = boxes.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        x_dist = (boxes_i[:, :, 0] - boxes_j[:, :, 0]).abs()\n",
    "        y_dist = (boxes_i[:, :, 1] - boxes_j[:, :, 1]).abs()\n",
    "        y_diff = boxes_i[:, :, 1] - boxes_j[:, :, 1]  # Vertical alignment\n",
    "        spatial_dists = torch.stack([x_dist, y_dist, y_diff], dim=-1)\n",
    "\n",
    "        pairs = torch.cat([concat_features, product_features, spatial_dists], dim=-1)\n",
    "        linking_logits = self.similarity_net(pairs.reshape(-1, feature_dim * 3 + 3))\n",
    "        linking_logits = linking_logits.reshape(batch_size, batch_size)\n",
    "\n",
    "        mask = torch.eye(batch_size, device=linking_logits.device)\n",
    "        linking_logits = linking_logits * (1 - mask) - 1e9 * mask  # Prevent self-linking\n",
    "        return linking_logits\n",
    "\n",
    "class DocumentTrainer:\n",
    "    def __init__(self, model, linking_predictor, tokenizer, label_encoder, device):\n",
    "        self.model = model\n",
    "        self.linking_predictor = linking_predictor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.linking_predictor.to(device)\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs=20, save_path=\"./models\"):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': self.model.parameters(), 'lr': 5e-5},\n",
    "            {'params': self.linking_predictor.parameters(), 'lr': 1e-5}\n",
    "        ], weight_decay=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "        label_criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "        order_criterion = nn.CrossEntropyLoss()  # Will compute weights dynamically\n",
    "        linking_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        best_val_f1 = 0.0\n",
    "        best_linking_threshold = 0.5\n",
    "        patience = 30\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            self.model.train()\n",
    "            self.linking_predictor.train()\n",
    "            train_loss, train_label_acc, train_linking_acc = 0, 0, 0\n",
    "            num_batches = 0\n",
    "\n",
    "            # Compute dynamic order weights\n",
    "            order_counts = train_loader.dataset.order_counts_dict\n",
    "            total_orders = sum(order_counts.values())\n",
    "            order_weights = torch.tensor([total_orders / (order_counts.get(i, 1) * 7) for i in range(7)], dtype=torch.float).to(self.device)\n",
    "            order_criterion.weight = order_weights\n",
    "\n",
    "            for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                box = batch['box'].to(self.device)\n",
    "                page = batch['page'].to(self.device)\n",
    "                order = batch['order'].to(self.device)\n",
    "                labels = torch.tensor(self.label_encoder.transform(batch['label']), dtype=torch.long).to(self.device)\n",
    "                linking_targets = batch['linking_targets'].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask, box, page)\n",
    "                label_loss = label_criterion(outputs['label_logits'], labels)\n",
    "                order_loss = order_criterion(outputs['order_pred'], order.long())\n",
    "                linking_logits = self.linking_predictor(outputs['linking_features'], box)\n",
    "                linking_loss = linking_criterion(linking_logits, linking_targets)\n",
    "\n",
    "                #Custom loss function that considers loss in classification of labels, mismatch in order and high priority to losses in linking/hierarchy\n",
    "                total_loss = label_loss + 0.5*order_loss + 5.0*linking_loss\n",
    "\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(list(self.model.parameters()) + list(self.linking_predictor.parameters()), 0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += total_loss.item()\n",
    "                pred_labels = torch.argmax(outputs['label_logits'], dim=1)\n",
    "                train_label_acc += (pred_labels == labels).float().mean().item()\n",
    "                linking_preds = torch.argmax(linking_logits, dim=1)\n",
    "                train_linking_acc += (linking_preds == linking_targets).float().mean().item()\n",
    "                num_batches += 1\n",
    "\n",
    "            scheduler.step()\n",
    "            val_loss, val_metrics = self.validate(val_loader, label_criterion, order_criterion, linking_criterion, best_linking_threshold)\n",
    "            best_linking_threshold = self.optimize_threshold(val_loader)\n",
    "\n",
    "            print(f\"Train Loss: {train_loss/num_batches:.4f}, Train Acc: {train_label_acc/num_batches:.4f}, Train Linking Acc: {train_linking_acc/num_batches:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Precision: {val_metrics['precision']:.4f}, Val Recall: {val_metrics['recall']:.4f}, Val F1: {val_metrics['f1']:.4f}, Val Linking Acc: {val_metrics['linking_acc']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}, Linking Threshold: {best_linking_threshold:.4f}\")\n",
    "\n",
    "            if val_metrics['f1'] > best_val_f1:\n",
    "                best_val_f1 = val_metrics['f1']\n",
    "                self.save_model(save_path)\n",
    "                print(\"Saved best model!\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "    def optimize_threshold(self, val_loader):\n",
    "        self.model.eval()\n",
    "        self.linking_predictor.eval()\n",
    "        all_linking_probs, all_linking_targets = [], []\n",
    "        thresholds = np.arange(0.3, 0.7, 0.05)\n",
    "        best_acc, best_threshold = 0.0, 0.5\n",
    "\n",
    "        #For attention mask, rather than directly predicting output from input, it multiplies input with a mask. \n",
    "        #And thus changing our focus to determining the mask rather than working on complex relationship between pure input and pure output\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                box = batch['box'].to(self.device)\n",
    "                page = batch['page'].to(self.device)\n",
    "                linking_targets = batch['linking_targets'].to(self.device)\n",
    "                outputs = self.model(input_ids, attention_mask, box, page)\n",
    "                linking_logits = self.linking_predictor(outputs['linking_features'], box)\n",
    "                linking_probs = torch.sigmoid(linking_logits)\n",
    "                all_linking_probs.extend(linking_probs.cpu().numpy())\n",
    "                all_linking_targets.extend(linking_targets.cpu().numpy())\n",
    "\n",
    "        all_linking_probs = np.array(all_linking_probs)\n",
    "        all_linking_targets = np.array(all_linking_targets)\n",
    "        for thresh in thresholds:\n",
    "            linking_preds = np.argmax(all_linking_probs, axis=1)\n",
    "            linking_preds[all_linking_probs.max(axis=1) < thresh] = 0  # No parent if below threshold\n",
    "            acc = sum(p == t for p, t in zip(linking_preds, all_linking_targets)) / len(all_linking_targets)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_threshold = thresh\n",
    "\n",
    "        return best_threshold\n",
    "\n",
    "    def validate(self, val_loader, label_criterion, order_criterion, linking_criterion, linking_threshold):\n",
    "        self.model.eval()\n",
    "        self.linking_predictor.eval()\n",
    "        val_loss, all_preds, all_labels, all_linking_preds, all_linking_targets = 0, [], [], [], []\n",
    "        num_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                box = batch['box'].to(self.device)\n",
    "                page = batch['page'].to(self.device)\n",
    "                order = batch['order'].to(self.device)\n",
    "                labels = torch.tensor(self.label_encoder.transform(batch['label']), dtype=torch.long).to(self.device)\n",
    "                linking_targets = batch['linking_targets'].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask, box, page)\n",
    "                label_loss = label_criterion(outputs['label_logits'], labels)\n",
    "                order_loss = order_criterion(outputs['order_pred'], order.long())\n",
    "                linking_logits = self.linking_predictor(outputs['linking_features'], box)\n",
    "                linking_loss = linking_criterion(linking_logits, linking_targets)\n",
    "\n",
    "                total_loss = label_loss + 0.5 * order_loss + 5.0 * linking_loss\n",
    "                val_loss += total_loss.item()\n",
    "                pred_labels = torch.argmax(outputs['label_logits'], dim=-1)\n",
    "                all_preds.extend(pred_labels.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                linking_preds = torch.argmax(linking_logits, dim=1)\n",
    "                all_linking_preds.extend(linking_preds.cpu().numpy())\n",
    "                all_linking_targets.extend(linking_targets.cpu().numpy())\n",
    "                num_batches += 1\n",
    "\n",
    "        val_loss = val_loss / num_batches\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        linking_acc = sum(p == t for p, t in zip(all_linking_preds, all_linking_targets)) / len(all_linking_targets)\n",
    "        accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)\n",
    "\n",
    "        return val_loss, {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'linking_acc': linking_acc,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        torch.save(self.model.state_dict(), os.path.join(save_path, \"document_model.pth\"))\n",
    "        torch.save(self.linking_predictor.state_dict(), os.path.join(save_path, \"linking_predictor.pth\"))\n",
    "        with open(os.path.join(save_path, \"label_encoder.pkl\"), 'wb') as f:\n",
    "            pickle.dump(self.label_encoder, f)\n",
    "\n",
    "class FastInferenceModel:\n",
    "    def __init__(self, model_path, linking_threshold=0.5):\n",
    "        self.device = torch.device('cpu')\n",
    "        self.linking_threshold = linking_threshold\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('./pretrained_models_bert_tiny')\n",
    "        with open(os.path.join(model_path, \"label_encoder.pkl\"), 'rb') as f:\n",
    "            self.label_encoder = pickle.load(f)\n",
    "\n",
    "        self.model = CompactDocumentModel(num_labels=len(self.label_encoder.classes_))\n",
    "        self.linking_predictor = ImprovedLinkingPredictor()\n",
    "        self.model.load_state_dict(torch.load(os.path.join(model_path, \"document_model.pth\"), map_location='cpu'))\n",
    "        self.linking_predictor.load_state_dict(torch.load(os.path.join(model_path, \"linking_predictor.pth\"), map_location='cpu'))\n",
    "        self.model.eval()\n",
    "        self.linking_predictor.eval()\n",
    "\n",
    "    def predict(self, input_file):\n",
    "        start_time = time.time()\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        items = data.get('contents', [])\n",
    "        if not items:\n",
    "            return []\n",
    "\n",
    "        texts, boxes, pages, item_ids = [], [], [], []\n",
    "        for item in items:\n",
    "            text = item.get('text', '') or '[EMPTY]'\n",
    "            texts.append(text)\n",
    "            page_num = item.get('page', 1)\n",
    "            page_info = data.get('pages', {}).get(f'page{page_num}', \n",
    "                                                 data.get('pages', {}).get(str(page_num), \n",
    "                                                 {'width': 1000, 'height': 1000}))\n",
    "            box = item.get('box', [0, 0, 0, 0])\n",
    "            normalized_box = [\n",
    "                box[0] / page_info['width'],\n",
    "                box[1] / page_info['height'],\n",
    "                box[2] / page_info['width'],\n",
    "                box[3] / page_info['height']\n",
    "            ]\n",
    "            boxes.append(normalized_box)\n",
    "            pages.append(page_num)\n",
    "            item_id = item.get('id', 0)\n",
    "            if item_id == 0:\n",
    "                continue\n",
    "            item_ids.append(item_id)\n",
    "\n",
    "        if not item_ids:\n",
    "            return []\n",
    "\n",
    "        encodings = self.tokenizer(texts, truncation=True, padding='max_length', max_length=64, return_tensors='pt')\n",
    "        input_ids = encodings['input_ids'].to(self.device)\n",
    "        attention_mask = encodings['attention_mask'].to(self.device)\n",
    "        box_tensor = torch.tensor(boxes, dtype=torch.float32).to(self.device)\n",
    "        page_tensor = torch.tensor(pages, dtype=torch.long).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask, box_tensor, page_tensor)\n",
    "            label_probs = torch.softmax(outputs['label_logits'], dim=-1)\n",
    "            label_preds = torch.argmax(label_probs, dim=-1)\n",
    "            order_preds = torch.argmax(outputs['order_pred'], dim=-1)\n",
    "            linking_logits = self.linking_predictor(outputs['linking_features'], box_tensor)\n",
    "            linking_probs = torch.sigmoid(linking_logits)\n",
    "\n",
    "        try:\n",
    "            with open(input_file.replace('input', 'output'), 'r', encoding='utf-8') as f:\n",
    "                gt_data = json.load(f)\n",
    "            gt_map = {item['id']: item for item in gt_data.get('contents', [])}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ground truth for {input_file}: {e}\")\n",
    "            gt_map = {}\n",
    "\n",
    "        results = []\n",
    "        for i in range(len(item_ids)):\n",
    "            pred_label = self.label_encoder.inverse_transform([label_preds[i].item()])[0]\n",
    "            if pred_label not in ['title', 'section-title']:\n",
    "                continue\n",
    "            max_prob, max_idx = torch.max(linking_probs[i], dim=0)\n",
    "            parent_id = item_ids[max_idx] if max_prob.item() > self.linking_threshold and max_idx != i and item_ids[max_idx] != item_ids[i] else 0\n",
    "            results.append({\n",
    "                'id': item_ids[i],\n",
    "                'label': pred_label,\n",
    "                'parent_id': parent_id,\n",
    "                # 'order': int(order_preds[i].item()),\n",
    "                'confidence': label_probs[i, label_preds[i]].item(),\n",
    "                'text': texts[i]\n",
    "                # 'gt_text': gt_map.get(item_ids[i], {}).get('text', '')\n",
    "            })\n",
    "\n",
    "        inference_time = time.time() - start_time\n",
    "        print(f\"Inference completed in {inference_time:.2f} seconds\")\n",
    "        return results\n",
    "\n",
    "class DocIENetPreprocessor:\n",
    "    def __init__(self, dataset_path=\"../datasets/dochienet_dataset/dochienet_dataset/labels\", input_dir=\"./input\", output_dir=\"./output\"):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(input_dir, exist_ok=True)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def split_dataset(self):\n",
    "        print(\"Processing DocIENet dataset...\")\n",
    "        if not os.path.exists(self.dataset_path):\n",
    "            print(f\"Dataset path {self.dataset_path} not found!\")\n",
    "            return [], []\n",
    "\n",
    "        input_files, output_files, label_counts, order_counts = [], [], {}, {}\n",
    "        print(\"Processing DocIENet dataset...\")\n",
    "        if not os.path.exists(self.dataset_path):\n",
    "            print(f\"Dataset path {self.dataset_path} not found!\")\n",
    "            return [], []\n",
    "\n",
    "        input_files, output_files, label_counts, order_counts = [], [], {}, {}\n",
    "        link_counts, discarded_links = 0, 0\n",
    "        for filename in tqdm(os.listdir(self.dataset_path)):\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(self.dataset_path, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    # Validate IDs\n",
    "                    valid_items = [item for item in data.get('contents', []) if item.get('label') in DocIENetDataset.VALID_LABELS]\n",
    "                    valid_ids = {item['id'] for item in valid_items}\n",
    "                    if not valid_ids:\n",
    "                        continue\n",
    "\n",
    "                    for item in data.get('contents', []):\n",
    "                        if item.get('id', 0) == 0:\n",
    "                            continue\n",
    "                        label = item.get('label', 'other')\n",
    "                        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "                        order = min(item.get('order', 0), 6)  # Cap at 6\n",
    "                        order_counts[order] = order_counts.get(order, 0) + 1\n",
    "\n",
    "                    input_data = {\"pages\": data.get(\"pages\", {}), \"contents\": []}\n",
    "                    output_data = {\"contents\": []}\n",
    "\n",
    "                    for item in data.get('contents', []):\n",
    "                        if item.get('id', 0) == 0:\n",
    "                            continue\n",
    "                        input_item = {\n",
    "                            \"box\": item.get(\"box\", [0, 0, 0, 0]),\n",
    "                            \"text\": item.get(\"text\", \"\") or \"[EMPTY]\",\n",
    "                            \"page\": item.get(\"page\", 1),\n",
    "                            \"id\": item.get(\"id\", 0)\n",
    "                        }\n",
    "                        input_data[\"contents\"].append(input_item)\n",
    "\n",
    "                    valid_items = sorted(valid_items, key=lambda x: x.get('order', 0))\n",
    "                    order_map = {item['id']: idx for idx, item in enumerate(valid_items)}\n",
    "\n",
    "                    for item in valid_items:\n",
    "                        parent_id = 0\n",
    "                        for link in item.get(\"linking\", []):\n",
    "                            if len(link) != 2:\n",
    "                                continue\n",
    "                            source_id, target_id = link\n",
    "                            if source_id not in valid_ids or target_id not in valid_ids or source_id == 0 or target_id == 0:\n",
    "                                discarded_links += 1\n",
    "                                continue\n",
    "                            if source_id == target_id:\n",
    "                                print(f\"Invalid self-link in {filename}: {link}\")\n",
    "                                discarded_links += 1\n",
    "                                continue\n",
    "                            parent_id = target_id\n",
    "                            link_counts += 1\n",
    "                            break\n",
    "\n",
    "                        output_item = {\n",
    "                            \"id\": item.get(\"id\", 0),\n",
    "                            \"label\": item.get(\"label\", \"other\"),\n",
    "                            \"text\": item.get(\"text\", \"\") or \"[EMPTY]\",\n",
    "                            \"parent_id\": parent_id,\n",
    "                            \"order\": min(order_map.get(item.get(\"id\", 0), 0), 6)\n",
    "                        }\n",
    "                        output_data[\"contents\"].append(output_item)\n",
    "\n",
    "                    if input_data[\"contents\"] and output_data[\"contents\"]:\n",
    "                        input_file_path = os.path.join(self.input_dir, filename)\n",
    "                        with open(input_file_path, 'w', encoding='utf-8') as f:\n",
    "                            json.dump(input_data, f, indent=2, ensure_ascii=False)\n",
    "                        input_files.append(input_file_path)\n",
    "                        output_file_path = os.path.join(self.output_dir, filename)\n",
    "                        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "                        output_files.append(output_file_path)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        print(f\"Processed {len(input_files)} files with valid labels\")\n",
    "        print(f\"Input files saved to: {self.input_dir}\")\n",
    "        print(f\"Output files saved to: {self.output_dir}\")\n",
    "        print(f\"Label distribution: {label_counts}\")\n",
    "        print(f\"Order distribution: {order_counts}\")\n",
    "        print(f\"Total valid links: {link_counts}\")\n",
    "        print(f\"Discarded links: {discarded_links}\")\n",
    "        return input_files, output_files\n",
    "\n",
    "def main():\n",
    "    print(\"=== Improved DocIENet Model Training (Titles & Section-Titles as Separate Labels) ===\")\n",
    "    preprocessor = DocIENetPreprocessor()\n",
    "    input_files, output_files = preprocessor.split_dataset()\n",
    "\n",
    "    if not input_files:\n",
    "        print(\"No files with valid labels processed. Please check your dataset path.\")\n",
    "        return\n",
    "\n",
    "    total_files = len(input_files)\n",
    "    train_size = int(0.7 * total_files)\n",
    "    val_size = int(0.1 * total_files)\n",
    "    print(f\"\\nDataset split:\")\n",
    "    print(f\"Total files: {total_files}\")\n",
    "    print(f\"Train files: {train_size} (70%)\")\n",
    "    print(f\"Validation files: {val_size} (10%)\")\n",
    "    print(f\"Unused files: {total_files - train_size - val_size} (20%)\")\n",
    "\n",
    "    train_files = input_files[:train_size]\n",
    "    train_output_files = output_files[:train_size]\n",
    "    val_files = input_files[train_size:train_size + val_size]\n",
    "    val_output_files = output_files[train_size:train_size + val_size]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./pretrained_models_bert_tiny')\n",
    "    train_dataset = DocIENetDataset(train_files, train_output_files, tokenizer)\n",
    "    val_dataset = DocIENetDataset(val_files, val_output_files, tokenizer, label_encoder=train_dataset.label_encoder)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)\n",
    "\n",
    "    print(f\"\\nFinal dataset statistics:\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Number of labels: {len(train_dataset.label_encoder.classes_)}\")\n",
    "    print(f\"Labels: {list(train_dataset.label_encoder.classes_)}\")\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "    model = CompactDocumentModel(num_labels=len(train_dataset.label_encoder.classes_))\n",
    "    linking_predictor = ImprovedLinkingPredictor()\n",
    "    total_params = sum(p.numel() for p in model.parameters()) + sum(p.numel() for p in linking_predictor.parameters())\n",
    "    print(f\"Total model parameters: {total_params / 1e6:.1f}M\")\n",
    "\n",
    "    trainer = DocumentTrainer(model, linking_predictor, tokenizer, train_dataset.label_encoder, device)\n",
    "    print(\"\\nStarting training...\")\n",
    "    trainer.train(train_loader, val_loader, num_epochs=50)\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(\"\\nTesting inference speed...\")\n",
    "    inference_model = FastInferenceModel(\"./models\")\n",
    "\n",
    "    if input_files:\n",
    "        val = int(0.9 * len(input_files))\n",
    "        test_file = input_files[val]\n",
    "        print(f\"Testing inference on {test_file}\")\n",
    "        results = inference_model.predict(test_file)\n",
    "        with open(test_file.replace('input', 'output'), 'r', encoding='utf-8') as f:\n",
    "            gt_data = json.load(f)\n",
    "        gt_map = {item['id']: item for item in gt_data.get('contents', [])}\n",
    "\n",
    "        print(\"\\nComparison of predictions vs ground truth:\")\n",
    "        for pred in results:\n",
    "            item_id = pred['id']\n",
    "            gt = gt_map.get(item_id)\n",
    "            if gt:\n",
    "                print(f\"ID: {item_id}\")\n",
    "                print(f\"  Pred label: {pred['label']}, GT label: {gt['label']}\")\n",
    "                print(f\"  Pred order: {pred['order']}, GT order: {gt['order']}\")\n",
    "                print(f\"  Pred parent_id: {pred['parent_id']}\")\n",
    "                print(f\"  GT parent_id: {gt['parent_id']}\")\n",
    "                print(f\"  Text: {pred['text']}\")\n",
    "                print(f\"  GT Text: {gt['text']}\")\n",
    "                print(\"---\")\n",
    "            else:\n",
    "                print(f\"ID {item_id} not found in ground truth.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastInferenceModel:\n",
    "    def __init__(self, model_path, linking_threshold=0.5):\n",
    "        self.device = torch.device('cpu')\n",
    "        self.linking_threshold = linking_threshold\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('./pretrained_models_bert_tiny')\n",
    "        with open(os.path.join(model_path, \"label_encoder.pkl\"), 'rb') as f:\n",
    "            self.label_encoder = pickle.load(f)\n",
    "\n",
    "        self.model = CompactDocumentModel(num_labels=len(self.label_encoder.classes_))\n",
    "        self.linking_predictor = ImprovedLinkingPredictor()\n",
    "        self.model.load_state_dict(torch.load(os.path.join(model_path, \"document_model.pth\"), map_location='cpu'))\n",
    "        self.linking_predictor.load_state_dict(torch.load(os.path.join(model_path, \"linking_predictor.pth\"), map_location='cpu'))\n",
    "        self.model.eval()\n",
    "        self.linking_predictor.eval()\n",
    "\n",
    "    def predict(self, input_file):\n",
    "        start_time = time.time()\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        items = data.get('contents', [])\n",
    "        if not items:\n",
    "            return []\n",
    "\n",
    "        texts, boxes, pages, item_ids = [], [], [], []\n",
    "        for item in items:\n",
    "            text = item.get('text', '') or '[EMPTY]'\n",
    "            texts.append(text)\n",
    "            page_num = item.get('page', 1)\n",
    "            page_info = data.get('pages', {}).get(f'page{page_num}', \n",
    "                                                 data.get('pages', {}).get(str(page_num), \n",
    "                                                 {'width': 1000, 'height': 1000}))\n",
    "            box = item.get('box', [0, 0, 0, 0])\n",
    "            normalized_box = [\n",
    "                box[0] / page_info['width'],\n",
    "                box[1] / page_info['height'],\n",
    "                box[2] / page_info['width'],\n",
    "                box[3] / page_info['height']\n",
    "            ]\n",
    "            boxes.append(normalized_box)\n",
    "            pages.append(page_num)\n",
    "            item_id = item.get('id', 0)\n",
    "            if item_id == 0:\n",
    "                continue\n",
    "            item_ids.append(item_id)\n",
    "\n",
    "        if not item_ids:\n",
    "            return []\n",
    "\n",
    "        encodings = self.tokenizer(texts, truncation=True, padding='max_length', max_length=64, return_tensors='pt')\n",
    "        input_ids = encodings['input_ids'].to(self.device)\n",
    "        attention_mask = encodings['attention_mask'].to(self.device)\n",
    "        box_tensor = torch.tensor(boxes, dtype=torch.float32).to(self.device)\n",
    "        page_tensor = torch.tensor(pages, dtype=torch.long).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask, box_tensor, page_tensor)\n",
    "            label_probs = torch.softmax(outputs['label_logits'], dim=-1)\n",
    "            label_preds = torch.argmax(label_probs, dim=-1)\n",
    "            order_preds = torch.argmax(outputs['order_pred'], dim=-1)\n",
    "            linking_logits = self.linking_predictor(outputs['linking_features'], box_tensor)\n",
    "            linking_probs = torch.sigmoid(linking_logits)\n",
    "\n",
    "        try:\n",
    "            with open(input_file.replace('input', 'output'), 'r', encoding='utf-8') as f:\n",
    "                gt_data = json.load(f)\n",
    "            gt_map = {item['id']: item for item in gt_data.get('contents', [])}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ground truth for {input_file}: {e}\")\n",
    "            gt_map = {}\n",
    "\n",
    "        results = []\n",
    "        for i in range(len(item_ids)):\n",
    "            pred_label = self.label_encoder.inverse_transform([label_preds[i].item()])[0]\n",
    "            if pred_label not in ['title', 'section-title']:\n",
    "                continue\n",
    "            max_prob, max_idx = torch.max(linking_probs[i], dim=0)\n",
    "            parent_id = item_ids[max_idx] if max_prob.item() > self.linking_threshold and max_idx != i and item_ids[max_idx] != item_ids[i] else 0\n",
    "            results.append({\n",
    "                'id': item_ids[i],\n",
    "                'label': pred_label,\n",
    "                'parent_id': parent_id,\n",
    "                # 'order': int(order_preds[i].item()),\n",
    "                'confidence': label_probs[i, label_preds[i]].item(),\n",
    "                'text': texts[i]\n",
    "                # 'gt_text': gt_map.get(item_ids[i], {}).get('text', '')\n",
    "            })\n",
    "\n",
    "        inference_time = time.time() - start_time\n",
    "        print(f\"Inference completed in {inference_time:.2f} seconds\")\n",
    "        return results\n",
    "\n",
    "def extract_paragraphs_with_ocr(pdf_path):\n",
    "    structured_blocks = []\n",
    "    page_sizes = {}\n",
    "    order = 0\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_idx, page in enumerate(pdf.pages, 0):\n",
    "            page_sizes[str(page_idx)] = {\"width\": page.width, \"height\": page.height}\n",
    "            lines = list(page.extract_text_lines())\n",
    "            # if lines and any(line.get(\"text\", \"\").strip() for line in lines) and len(lines) >= 5:\n",
    "            for line in lines:\n",
    "                box = [line[\"x0\"], line[\"top\"], line[\"x1\"], line[\"bottom\"]]\n",
    "                if any(coord < 0 or coord > max(page.width, page.height) for coord in box):\n",
    "                    continue\n",
    "                block = {\n",
    "                    \"box\": box,\n",
    "                    \"text\": line[\"text\"] or \"[EMPTY]\",\n",
    "                    \"page\": page_idx,\n",
    "                    \"id\": order,\n",
    "                    \"order\": order\n",
    "                }\n",
    "                structured_blocks.append(block)\n",
    "                order += 1\n",
    "            # else:\n",
    "            #     try:\n",
    "            #         pil_img = page.to_image(resolution=300).original\n",
    "            #         ocr_text = pytesseract.image_to_string(pil_img, lang='eng')\n",
    "            #     except Exception as e:\n",
    "            #         print(f\"OCR failed for page {page_idx} in {pdf_path}: {e}\")\n",
    "            #         ocr_text = \"[EMPTY]\"\n",
    "            #     block = {\n",
    "            #         \"box\": [0, 0, page.width, page.height],\n",
    "            #         \"text\": ocr_text or \"[EMPTY]\",\n",
    "            #         \"page\": page_idx,\n",
    "            #         \"id\": order,\n",
    "            #         \"order\": order\n",
    "            #     }\n",
    "            #     structured_blocks.append(block)\n",
    "            #     order += 1\n",
    "\n",
    "    return {\n",
    "        # \"pages\": page_sizes,\n",
    "        \"contents\": structured_blocks\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2af5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: file01.pdf -> ./json_input\\file01.json\n",
      "Processed: file02.pdf -> ./json_input\\file02.json\n",
      "Processed: file03.pdf -> ./json_input\\file03.json\n",
      "Processed: file04.pdf -> ./json_input\\file04.json\n",
      "Processed: file05.pdf -> ./json_input\\file05.json\n",
      "=== DocIENet Model Inference for PDF Input ===\n",
      "\n",
      "Using device: cpu\n",
      "Testing inference on ./json_input\\file01.json\n",
      "Error loading ground truth for ./json_input\\file01.json: [Errno 2] No such file or directory: './json_output\\\\file01.json'\n",
      "Inference completed in 0.08 seconds\n",
      "Saved 22 predictions to ./pdf_output\\file01_predictions.json\n",
      "Testing inference on ./json_input\\file02.json\n",
      "Error loading ground truth for ./json_input\\file02.json: [Errno 2] No such file or directory: './json_output\\\\file02.json'\n",
      "Inference completed in 0.28 seconds\n",
      "Saved 220 predictions to ./pdf_output\\file02_predictions.json\n",
      "Testing inference on ./json_input\\file03.json\n",
      "Error loading ground truth for ./json_input\\file03.json: [Errno 2] No such file or directory: './json_output\\\\file03.json'\n",
      "Inference completed in 0.48 seconds\n",
      "Saved 346 predictions to ./pdf_output\\file03_predictions.json\n",
      "Testing inference on ./json_input\\file04.json\n",
      "Error loading ground truth for ./json_input\\file04.json: [Errno 2] No such file or directory: './json_output\\\\file04.json'\n",
      "Inference completed in 0.11 seconds\n",
      "Saved 22 predictions to ./pdf_output\\file04_predictions.json\n",
      "Testing inference on ./json_input\\file05.json\n",
      "Error loading ground truth for ./json_input\\file05.json: [Errno 2] No such file or directory: './json_output\\\\file05.json'\n",
      "Inference completed in 0.02 seconds\n",
      "Saved 1 predictions to ./pdf_output\\file05_predictions.json\n"
     ]
    }
   ],
   "source": [
    "test_pdf_dir = './pdf_input'\n",
    "test_output_dir = './json_input'\n",
    "os.makedirs(test_output_dir, exist_ok=True)\n",
    "\n",
    "for fname in os.listdir(test_pdf_dir):\n",
    "    if fname.lower().endswith('.pdf'):\n",
    "        in_pdf = os.path.join(test_pdf_dir, fname)\n",
    "        out_json = os.path.join(test_output_dir, os.path.splitext(fname)[0] + \".json\")\n",
    "        doc = extract_paragraphs_with_ocr(in_pdf)\n",
    "        with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(doc, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Processed: {fname} -> {out_json}\")\n",
    "\n",
    "def main():\n",
    "    print(\"=== DocIENet Model Inference for PDF Input ===\")\n",
    "    input_dir = \"./json_input\"\n",
    "    output_dir = \"./pdf_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    input_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.json') and not f.endswith('_predictions.json')]\n",
    "\n",
    "    if not input_files:\n",
    "        print(\"No JSON files found in ./json_input. Please run preprocess_pdf.py.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('./pretrained_models_bert_tiny')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load tokenizer from ./bert-tiny: {e}\")\n",
    "        return\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "    try:\n",
    "        inference_model = FastInferenceModel(\"./models\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize FastInferenceModel: {e}\")\n",
    "        return\n",
    "\n",
    "    for input_file in input_files:\n",
    "        print(f\"Testing inference on {input_file}\")\n",
    "        try:\n",
    "            results = inference_model.predict(input_file)\n",
    "            # Filter predictions with confidence > 0.8\n",
    "            filtered_results = [pred for pred in results if pred['confidence'] > 0.7]\n",
    "            output_file = os.path.join(output_dir, os.path.basename(input_file).replace('.json', '_predictions.json'))\n",
    "            if filtered_results:\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(filtered_results, f, indent=2, ensure_ascii=False)\n",
    "                print(f\"Saved {len(filtered_results)} predictions to {output_file}\")\n",
    "            else:\n",
    "                print(f\"No predictions with confidence > 0.8 for {input_file}. Skipping output.\")\n",
    "            # print(\"\\nPredictions:\")\n",
    "            # for pred in results:\n",
    "            #     print(f\"ID: {pred['id']}\")\n",
    "            #     print(f\"  Pred label: {pred['label']}\")\n",
    "            #     print(f\"  Pred order: {pred['order']}\")\n",
    "            #     print(f\"  Pred parent_id: {pred['parent_id']}\")\n",
    "            #     print(f\"  Text: {pred['text']}\")\n",
    "            #     print(f\"  Confidence: {pred['confidence']:.4f}\")\n",
    "            #     print(\"---\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {input_file}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
