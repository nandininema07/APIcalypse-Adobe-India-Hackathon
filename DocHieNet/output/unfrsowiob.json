{
  "contents": [
    {
      "id": 1,
      "label": "title",
      "text": "10Sent: A Stable Sentiment Analysis Metho d Based on the ",
      "parent_id": 0,
      "order": 0
    },
    {
      "id": 2,
      "label": "title",
      "text": "Combination of Off-The-Shelf Approaches ",
      "parent_id": 0,
      "order": 1
    },
    {
      "id": 7,
      "label": "section-title",
      "text": "Abstract ",
      "parent_id": 7,
      "order": 2
    },
    {
      "id": 9,
      "label": "section-title",
      "text": "1 Introduction ",
      "parent_id": 9,
      "order": 3
    },
    {
      "id": 23,
      "label": "section-title",
      "text": "2 Related Work ",
      "parent_id": 23,
      "order": 4
    },
    {
      "id": 37,
      "label": "section-title",
      "text": "3 Combining Methods ",
      "parent_id": 37,
      "order": 5
    },
    {
      "id": 51,
      "label": "section-title",
      "text": "4 Methodology ",
      "parent_id": 51,
      "order": 6
    },
    {
      "id": 63,
      "label": "section-title",
      "text": "5 Experimental Results ",
      "parent_id": 63,
      "order": 6
    },
    {
      "id": 65,
      "label": "section-title",
      "text": "5.1 Choice of The Classifier 10SENT is an unsupervised machine learning method as it does not exploit manually labeled data, only the agreement among the base methods. Given that the bootstrapping process adds a set of instances with high confidence into a training set, it is p ossible to perform a learning step exploiting such data in the the usual format training/validation. Because of this, there is a need to investigate which classifier fits better this application. Thus, we perform a series of tests with our method using different classification algorithms in order to cho ose the best one for this task. In all these tests, we used all 10 methods of 10SENT. We tested three different and widely used algorithms in our approach: Support Vector Machine (SVM) [Chang and Lin, 2011], Random Forest (RF) [Breiman, 2001] and k-Nearest Neighbors (KNN) [Pedregosa et al., 2011]. Here we used the implementations of RF and KNN provided in scikit-learn4 and for SVM, we use LibSVM5 package. Sp ecifically, we use a radial basis function (RBF) kernel with a grid search for the best parameters. Overall, Random Forests produced the best results in most datasets, being the final choice for our b ootstrapping metho d. ",
      "parent_id": 65,
      "order": 6
    },
    {
      "id": 66,
      "label": "section-title",
      "text": "5.2 Choice of Number of Methods ",
      "parent_id": 66,
      "order": 6
    },
    {
      "id": 72,
      "label": "section-title",
      "text": "5.3 Choice of Parameters ",
      "parent_id": 72,
      "order": 6
    },
    {
      "id": 80,
      "label": "section-title",
      "text": "5.4 Bag of Words vs. Predictions ",
      "parent_id": 80,
      "order": 6
    },
    {
      "id": 89,
      "label": "section-title",
      "text": "5.5 Transfer Learning Analysis ",
      "parent_id": 89,
      "order": 6
    },
    {
      "id": 102,
      "label": "section-title",
      "text": "6 Comparative Results ",
      "parent_id": 102,
      "order": 6
    },
    {
      "id": 108,
      "label": "section-title",
      "text": "6.1 Upp erBound Comparison ",
      "parent_id": 108,
      "order": 6
    },
    {
      "id": 122,
      "label": "section-title",
      "text": "6.1.1 Upp erbound Results ",
      "parent_id": 122,
      "order": 6
    },
    {
      "id": 139,
      "label": "section-title",
      "text": "7 Conclusions ",
      "parent_id": 139,
      "order": 6
    },
    {
      "id": 141,
      "label": "section-title",
      "text": "References ",
      "parent_id": 141,
      "order": 6
    }
  ]
}